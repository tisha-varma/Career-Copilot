# Career Copilot AI - Video Demonstration Script
## Complete Technical Walkthrough for Recorded Presentation

---

## üé¨ VIDEO SCRIPT

### SECTION 1: INTRODUCTION (1-2 minutes)
---

**[SLIDE: Title Screen - "Career Copilot AI"]**

> "Hello everyone! Today I'm going to walk you through Career Copilot AI - an intelligent resume analysis and career guidance platform that I've built using cutting-edge AI technologies.

> This isn't just another resume parser. It's a complete career intelligence system that uses Large Language Models, vector embeddings, and semantic search to provide truly personalized career guidance.

> By the end of this demo, you'll see how we leverage:
> - LLaMA 3.1 - a 70 billion parameter language model
> - BGE Embeddings for semantic understanding
> - ChromaDB for vector storage
> - And multiple APIs working together in a multi-step agentic pipeline"

---

### SECTION 2: ARCHITECTURE OVERVIEW (3-4 minutes)
---

**[SLIDE: System Architecture Diagram]**

> "Let me start by explaining the high-level architecture of Career Copilot AI.

> **The system has three main layers:**

> **1. Frontend Layer**
> - Built with FastAPI and Jinja2 templating
> - Styled using Tailwind CSS for a modern, responsive UI
> - Four main pages: Upload, Results, Jobs, Interview Prep, and Cover Letter Generator

> **2. AI Processing Layer - This is where the magic happens**
> - We use a multi-model approach for reliability
> - Primary: LLaMA 3.1 70B via Groq API
> - Secondary: Google Gemini 1.5 Flash
> - Fallback: Rule-based demo mode

> **3. Vector Database Layer**
> - BGE-Large embeddings with 1024 dimensions
> - ChromaDB for in-memory vector storage
> - Enables semantic similarity search on resume content"

**[SLIDE: Data Flow Diagram]**

> "Here's how data flows through the system:

```
Resume PDF ‚Üí pdfplumber ‚Üí Raw Text ‚Üí BGE Embeddings ‚Üí ChromaDB
                                           ‚Üì
Job Description ‚Üí Semantic Search ‚Üí Find Relevant Sections
                                           ‚Üì
                    LLaMA 3.1 ‚Üí Structured JSON Output
                                           ‚Üì
               Cover Letter / Interview Questions / Analysis
```

> The key innovation here is that we don't just dump the entire resume into the LLM. Instead, we use semantic search to find the MOST RELEVANT sections first, then feed only those to LLaMA. This improves accuracy and reduces token usage."

---

### SECTION 3: CORE TECHNOLOGIES DEEP DIVE (5-6 minutes)
---

**[SLIDE: LLaMA 3.1 Integration]**

> "Let's dive into the technical components, starting with our LLM integration.

> **LLaMA 3.1 70B via Groq**
> - We access LLaMA through Groq's inference API
> - Groq uses custom LPU (Language Processing Unit) hardware
> - This gives us inference speeds 10x faster than traditional GPU
> - The 70B parameter model provides excellent reasoning capabilities

> Here's how we call LLaMA in our code:"

```python
from groq import Groq

def call_llama(prompt: str, system_prompt: str = None) -> str:
    client = Groq(api_key=os.environ.get("GROQ_API_KEY"))
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    response = client.chat.completions.create(
        model="llama-3.1-70b-versatile",
        messages=messages,
        temperature=0.7,
        max_tokens=2048
    )
    
    return response.choices[0].message.content
```

**[SLIDE: BGE Embeddings]**

> "Next, let's talk about embeddings.

> **BGE-Large-en-v1.5**
> - BGE stands for 'BAAI General Embedding'
> - Produces 1024-dimensional dense vectors
> - Trained on massive text corpora for semantic understanding
> - We use it via Hugging Face's sentence-transformers library

> The embedding model converts text like 'Developed a machine learning model for threat detection' into a 1024-dimensional vector. Similar concepts end up close together in this vector space.

> Here's the code:"

```python
from sentence_transformers import SentenceTransformer

# Load the model once
model = SentenceTransformer('BAAI/bge-large-en-v1.5')

# Create embeddings
resume_chunks = ["Built ML models...", "Python, TensorFlow...", ...]
embeddings = model.encode(resume_chunks)  # Returns 1024-dim vectors
```

**[SLIDE: ChromaDB Vector Store]**

> "For storing and searching these embeddings, we use ChromaDB.

> **ChromaDB Features:**
> - In-memory vector database
> - Supports cosine similarity search
> - No external server required
> - Perfect for real-time applications

> When a user enters a job description, we:
> 1. Embed the job description
> 2. Search ChromaDB for similar resume chunks
> 3. Return the top 5 most relevant sections

> Here's the semantic search code:"

```python
import chromadb

# Create collection and add resume embeddings
collection = client.create_collection("resume_chunks")
collection.add(
    documents=chunks,
    embeddings=embeddings,
    ids=[f"chunk_{i}" for i in range(len(chunks))]
)

# Semantic search
results = collection.query(
    query_embeddings=model.encode(["Python ML experience"]),
    n_results=5
)
# Returns most semantically similar resume sections
```

---

### SECTION 4: AGENTIC AI PIPELINE (4-5 minutes)
---

**[SLIDE: Multi-Step Agent Architecture]**

> "Now let's look at how we combine these technologies into an agentic AI system.

> **What makes it 'Agentic'?**
> - Multi-step reasoning pipeline
> - Each step builds on previous results
> - Self-reflection and quality checks
> - Context passing between steps

> Our main analysis pipeline has 4 steps:"

```
Step 1: RESUME UNDERSTANDING
‚îú‚îÄ‚îÄ Input: Raw resume text
‚îú‚îÄ‚îÄ LLM extracts: skills, education, experience, strengths
‚îî‚îÄ‚îÄ Output: Structured JSON

Step 2: ROLE FIT ANALYSIS
‚îú‚îÄ‚îÄ Input: Extracted skills + Target role requirements
‚îú‚îÄ‚îÄ LLM compares and identifies gaps
‚îî‚îÄ‚îÄ Output: Fit score (0-100), missing skills

Step 3: LEARNING ROADMAP
‚îú‚îÄ‚îÄ Input: Missing skills
‚îú‚îÄ‚îÄ LLM creates prioritized learning plan
‚îî‚îÄ‚îÄ Output: Ordered skill recommendations with timeframes

Step 4: REFLECTION (Self-evaluation)
‚îú‚îÄ‚îÄ Input: Score + Roadmap
‚îú‚îÄ‚îÄ LLM evaluates: "Is this roadmap sufficient?"
‚îî‚îÄ‚îÄ Output: Status (sufficient/insufficient) + reasoning
```

> "This is what distinguishes agentic AI from simple chatbots. The system reasons across multiple steps and can evaluate its own outputs."

**[SLIDE: Cover Letter Generation Flow]**

> "For cover letter generation, we use a RAG-like approach:

> **RAG = Retrieval Augmented Generation**

```
1. RETRIEVAL PHASE
   - Embed job description
   - Search ChromaDB for relevant resume sections
   - Find: matching projects, skills, experience

2. AUGMENTATION PHASE
   - Combine relevant sections with job context
   - Build structured prompt for LLaMA

3. GENERATION PHASE
   - LLaMA generates personalized cover letter
   - References SPECIFIC projects by name
   - Includes ACTUAL technologies from resume
   - Mentions REAL achievements
```

> This ensures the cover letter is grounded in the actual resume content, not hallucinated."

---

### SECTION 5: LIVE DEMO (5-7 minutes)
---

**[SCREEN: Show browser at http://127.0.0.1:8000]**

> "Now let me show you Career Copilot AI in action.

> **Demo 1: Resume Analysis**
> 1. I'll upload a sample resume PDF
> 2. Select 'Machine Learning Engineer' as the target role
> 3. Click 'Analyze Resume'

> [Wait for analysis]

> Notice the results page shows:
> - Role Fit Score with animated progress bar
> - Detected strengths pulled from the resume
> - Skill gaps - what's missing for this role
> - Personalized learning roadmap
> - YouTube video recommendations
> - AI reflection on the overall assessment

**[SCREEN: Navigate to Cover Letter Generator]**

> **Demo 2: Cover Letter Generation**
> 1. I'll click 'Cover Letter Generator'
> 2. Enter company name: 'Google'
> 3. Position: 'Machine Learning Engineer'
> 4. Paste a sample job description

> [Wait for generation]

> See how the cover letter:
> - Mentions SPECIFIC projects by name (HerDefenders, PharmaMate)
> - References ACTUAL technologies (Python, OpenCV, TensorFlow)
> - Includes REAL achievements (Google Girl Hackathon Finalist)
> - Tailored to the job description

> This is the power of semantic search + LLaMA working together.

**[SCREEN: Navigate to Interview Prep]**

> **Demo 3: Interview Questions**
> Here you can see:
> - Questions about specific projects from the resume
> - Technical questions based on listed skills
> - Behavioral questions with context-aware tips

> Each question has a 'source' field showing which resume section it came from."

---

### SECTION 6: CODE WALKTHROUGH (3-4 minutes)
---

**[SCREEN: Show VS Code with project structure]**

> "Let me quickly walk through the project structure:

```
career-copilot-ai/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py           # FastAPI routes
‚îÇ   ‚îú‚îÄ‚îÄ agent.py          # Gemini-based analysis
‚îÇ   ‚îú‚îÄ‚îÄ llama_analyzer.py # LLaMA + embeddings
‚îÇ   ‚îú‚îÄ‚îÄ cover_letter.py   # Cover letter generation
‚îÇ   ‚îú‚îÄ‚îÄ resume_analyzer.py# Interview questions
‚îÇ   ‚îú‚îÄ‚îÄ resume_parser.py  # PDF text extraction
‚îÇ   ‚îú‚îÄ‚îÄ pdf_generator.py  # ReportLab PDF output
‚îÇ   ‚îú‚îÄ‚îÄ templates/        # Jinja2 HTML templates
‚îÇ   ‚îî‚îÄ‚îÄ static/           # CSS, images
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

> **Key Files:**

> `llama_analyzer.py` - The heart of our semantic analysis:
> - Creates BGE embeddings
> - Stores in ChromaDB
> - Performs semantic search
> - Calls LLaMA for generation

> `agent.py` - The 4-step analysis pipeline:
> - Resume understanding
> - Role fit analysis
> - Learning roadmap
> - Reflection

> `main.py` - FastAPI application:
> - Routes for all features
> - Session management
> - Template rendering"

---

### SECTION 7: TECHNICAL CHALLENGES & SOLUTIONS (2 minutes)
---

**[SLIDE: Challenges Overcome]**

> "During development, I faced several interesting challenges:

> **Challenge 1: Hallucination**
> - Problem: LLMs invented skills not in the resume
> - Solution: Semantic search for grounding + strict prompts

> **Challenge 2: Context Window Limits**
> - Problem: Long resumes exceed token limits
> - Solution: Chunking + relevance filtering through embeddings

> **Challenge 3: API Reliability**
> - Problem: APIs can fail or rate-limit
> - Solution: Multi-model fallback (LLaMA ‚Üí Gemini ‚Üí Demo mode)

> **Challenge 4: Accuracy**
> - Problem: Regex-based extraction was inaccurate
> - Solution: LLM-based structured extraction with JSON output"

---

### SECTION 8: CONCLUSION (1 minute)
---

**[SLIDE: Summary]**

> "To summarize, Career Copilot AI demonstrates:

> ‚úÖ Multi-model LLM integration (LLaMA 3.1, Gemini)
> ‚úÖ Vector embeddings for semantic understanding
> ‚úÖ Agentic multi-step reasoning
> ‚úÖ RAG architecture for grounded generation
> ‚úÖ Production-ready fallback mechanisms

> **Tech Stack Recap:**
> - Backend: FastAPI, Python
> - LLM: LLaMA 3.1 70B (Groq), Gemini 1.5
> - Embeddings: BGE-Large
> - Vector DB: ChromaDB
> - PDF: pdfplumber, ReportLab
> - Frontend: Tailwind CSS, Jinja2

> Thank you for watching! The code is available for review, and I'm happy to answer any questions about the implementation."

**[SLIDE: Thank You]**

---

## üìä QUICK FACTS FOR Q&A

| Component | Technology | Why? |
|-----------|------------|------|
| LLM | LLaMA 3.1 70B | Best open-source reasoning |
| Inference | Groq API | 10x faster than GPU |
| Embeddings | BGE-Large | SOTA semantic similarity |
| Vector DB | ChromaDB | Simple, in-memory, fast |
| PDF Parsing | pdfplumber | Accurate text extraction |
| Web Framework | FastAPI | Async, fast, modern |

---

## üéØ KEY TALKING POINTS

1. **Agentic vs Regular AI**: Multi-step reasoning, self-reflection, context passing
2. **RAG Architecture**: Retrieval ‚Üí Augment ‚Üí Generate for grounded outputs
3. **Semantic Search**: Find relevant resume sections, not keyword matching
4. **Multi-Model Fallback**: LLaMA ‚Üí Gemini ‚Üí Demo for reliability
5. **Vector Embeddings**: 1024-dim dense vectors for similarity matching

---

## ‚è±Ô∏è TIMING GUIDE

| Section | Duration |
|---------|----------|
| Introduction | 1-2 min |
| Architecture | 3-4 min |
| Tech Deep Dive | 5-6 min |
| Agentic Pipeline | 4-5 min |
| Live Demo | 5-7 min |
| Code Walkthrough | 3-4 min |
| Challenges | 2 min |
| Conclusion | 1 min |
| **TOTAL** | **25-30 min** |

---
